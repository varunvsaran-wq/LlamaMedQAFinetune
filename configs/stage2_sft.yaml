### Stage 2: Supervised Fine-Tuning on MedQA ###

# Model
model_name_or_path: meta-llama/Meta-Llama-3-8B
adapter_name_or_path: saves/llama3-8b-medqa/lora/pretrain
quantization_bit: 4
quantization_method: bitsandbytes

# Data
stage: sft
dataset_dir: data
dataset: medqa_sft
template: llama3
cutoff_len: 1024
preprocessing_num_workers: 4

# LoRA
finetuning_type: lora
lora_rank: 16
lora_alpha: 32
lora_target: all
lora_dropout: 0.05

# Training
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 1.0e-4
num_train_epochs: 3
lr_scheduler_type: cosine
warmup_ratio: 0.05
max_grad_norm: 1.0

# Memory optimization
gradient_checkpointing: true
optim: paged_adamw_8bit
bf16: true

# Logging & saving
logging_steps: 10
save_steps: 500
save_total_limit: 2
output_dir: saves/llama3-8b-medqa/lora/sft
report_to: none
